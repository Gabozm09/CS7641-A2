{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrose_hiive as mr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the problem for 4-Peaks\n",
    "problem_size = 100 \n",
    "fitness = mr.FourPeaks(t_pct=0.1)\n",
    "problem_4peaks = mr.DiscreteOpt(length=problem_size, fitness_fn=fitness, maximize=True, max_val=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_attempts = 200\n",
    "max_iterations = 1000  # Limit iterations for quicker execution\n",
    "num_runs = 10  # Number of runs for statistical robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "results = {}\n",
    "\n",
    "# List of algorithms and their specific parameters\n",
    "algorithms = {\n",
    "    'random_hill_climb': {'restarts': 50},\n",
    "    'simulated_annealing': {'schedule': mr.ExpDecay(init_temp=1, exp_const=0.005, min_temp=0.001)},\n",
    "    'genetic_alg': {'pop_size': 200, 'mutation_prob': 0.1},\n",
    "    'mimic': {'pop_size': 200, 'keep_pct': 0.2}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute each algorithm multiple times\n",
    "for name, params in algorithms.items():\n",
    "    run_times = []\n",
    "    fitness_scores = []\n",
    "    curves = []\n",
    "    total_fevs = []  # List to store total function evaluations for each run\n",
    "\n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        _, best_fitness, fitness_curve = getattr(mr, name.lower())(\n",
    "            problem_4peaks, max_attempts=max_attempts, max_iters=max_iterations, \n",
    "            curve=True, random_state=42, **params)\n",
    "        end_time = time.time()\n",
    "\n",
    "        run_times.append(end_time - start_time)\n",
    "        fitness_scores.append(best_fitness)\n",
    "        curves.append(fitness_curve[:, 0])  # Assuming the first column is the fitness score\n",
    "\n",
    "        # Assuming the second column of fitness_curve contains the cumulative fevals\n",
    "        total_fevs.append(fitness_curve[-1, 1])\n",
    "\n",
    "    # Calculate fevals per second and per iteration\n",
    "    fevals_per_sec = sum(total_fevs) / sum(run_times)\n",
    "    fevals_per_iteration = sum(total_fevs) / (num_runs * max_iterations)\n",
    "\n",
    "    results[name] = {\n",
    "        'times': run_times,\n",
    "        'fitness_scores': fitness_scores,\n",
    "        'average_curve': np.mean(curves, axis=0),\n",
    "        'total_fevs': sum(total_fevs),  # Total function evaluations across all runs\n",
    "        'fevals_per_sec': fevals_per_sec,  # Function evaluations per second\n",
    "        'fevals_per_iteration': fevals_per_iteration  # Function evaluations per iteration\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('4peaks_results_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot average fitness curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "for algo, data in results.items():\n",
    "    plt.plot(data['average_curve'], label=f\"{algo} (Avg. Fitness)\")\n",
    "plt.title('Average Fitness Curves for 4-Peaks Problem')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Fitness')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot convergence times\n",
    "plt.figure(figsize=(10, 6))\n",
    "convergence_times = [np.mean(data['times']) for algo, data in results.items()]\n",
    "plt.bar(results.keys(), convergence_times, color='lightblue')\n",
    "plt.title('Average Convergence Times for 4-Peaks Problem')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Time in seconds')\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved parsing function that checks the data type\n",
    "import json\n",
    "\n",
    "\n",
    "def parse_list(data):\n",
    "    if isinstance(data, str):\n",
    "        try:\n",
    "            return json.loads(data.replace('\\n', '').replace('  ', ',').replace('array(', '[').replace(')', ']'))\n",
    "        except json.JSONDecodeError:\n",
    "            return eval(data)  # Using eval as a fallback for string representations of lists\n",
    "    return data  # Return as-is if it's already a list\n",
    "# Setting a style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Define a color palette\n",
    "palette = sns.color_palette(\"muted\")\n",
    "\n",
    "# Now let's extract and convert the data correctly\n",
    "try:\n",
    "    rhc_times = parse_list(results_df.iloc[0]['random_hill_climb'])\n",
    "    sa_times = parse_list(results_df.iloc[0]['simulated_annealing'])\n",
    "    ga_times = parse_list(results_df.iloc[0]['genetic_alg'])\n",
    "    mimic_times = parse_list(results_df.iloc[0]['mimic'])\n",
    "\n",
    "    rhc_curve = parse_list(results_df.iloc[2]['random_hill_climb'])\n",
    "    sa_curve = parse_list(results_df.iloc[2]['simulated_annealing'])\n",
    "    ga_curve = parse_list(results_df.iloc[2]['genetic_alg'])\n",
    "    mimic_curve = parse_list(results_df.iloc[2]['mimic'])\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [np.mean(rhc_times), np.mean(sa_times), np.mean(ga_times), np.mean(mimic_times)]\n",
    "labels = ['RHC', 'SA', 'GA', 'MIMIC']\n",
    "\n",
    "time_data = {\n",
    "    'Algorithm': labels,\n",
    "    'Mean Execution Time (seconds)': times,\n",
    "    'Standard Deviation': [np.std(rhc_times), np.std(sa_times), np.std(ga_times), np.std(mimic_times)]\n",
    "}\n",
    "\n",
    "execution_times_df = pd.DataFrame(time_data)\n",
    "\n",
    "# Function to highlight the minimum execution time\n",
    "def highlight_min(s):\n",
    "    is_min = s == min(s)\n",
    "    return ['background-color: yellow' if v else '' for v in is_min]\n",
    "\n",
    "# Styling the DataFrame\n",
    "styled_df = execution_times_df.style.apply(highlight_min, subset=['Mean Execution Time (seconds)'])\\\n",
    "                                    .format({'Mean Execution Time (seconds)': \"{:.2f}\", 'Standard Deviation': \"{:.2f}\"})\\\n",
    "                                    .set_table_styles([{'selector': 'th', 'props': [('font-size', '12pt')]}])\\\n",
    "                                    .set_properties(**{'font-size': '11pt'})\\\n",
    "                                    .set_caption(\"Summary of Execution Times\")\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Parse the average curves (Fitness/Iteration)\n",
    "results_df['random_hill_climb'][2] = parse_list(results_df['random_hill_climb'][2])\n",
    "results_df['simulated_annealing'][2] = parse_list(results_df['simulated_annealing'][2])\n",
    "results_df['genetic_alg'][2] = parse_list(results_df['genetic_alg'][2])\n",
    "results_df['mimic'][2] = parse_list(results_df['mimic'][2])\n",
    "\n",
    "# Plot Fitness / Iteration for each algorithm\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(results_df['random_hill_climb'][2], label='Random Hill Climb')\n",
    "plt.plot(results_df['simulated_annealing'][2], label='Simulated Annealing')\n",
    "plt.plot(results_df['genetic_alg'][2], label='Genetic Algorithm')\n",
    "plt.plot(results_df['mimic'][2], label='MIMIC')\n",
    "plt.title('Fitness / Iteration (Average Curve)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Fitness')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Parse the wall clock times\n",
    "results_df['random_hill_climb'][0] = parse_list(results_df['random_hill_climb'][0])\n",
    "results_df['simulated_annealing'][0] = parse_list(results_df['simulated_annealing'][0])\n",
    "results_df['genetic_alg'][0] = parse_list(results_df['genetic_alg'][0])\n",
    "results_df['mimic'][0] = parse_list(results_df['mimic'][0])\n",
    "\n",
    "# Calculate total wall clock time for each algorithm\n",
    "total_times = {\n",
    "    'Random Hill Climb': np.sum(results_df['random_hill_climb'][0]),\n",
    "    'Simulated Annealing': np.sum(results_df['simulated_annealing'][0]),\n",
    "    'Genetic Algorithm': np.sum(results_df['genetic_alg'][0]),\n",
    "    'MIMIC': np.sum(results_df['mimic'][0])\n",
    "}\n",
    "\n",
    "# Plot Wall Clock Time for each algorithm\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(total_times.keys(), total_times.values(), color=['blue', 'green', 'red', 'purple'])\n",
    "plt.title('Wall Clock Time per Algorithm')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Total Time (seconds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and Plot Function Evaluations Metrics\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(results)), [val['total_fevs'] for val in results.values()], color='blue', align='center')\n",
    "plt.xticks(range(len(results)), list(results.keys()))\n",
    "plt.title('Total Function Evaluations per Algorithm')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Total Function Evaluations')\n",
    "plt.show()\n",
    "\n",
    "# Function Evaluations per Second\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(results)), [val['fevals_per_sec'] for val in results.values()], color='green', align='center')\n",
    "plt.xticks(range(len(results)), list(results.keys()))\n",
    "plt.title('Function Evaluations per Second per Algorithm')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Function Evaluations per Second')\n",
    "plt.show()\n",
    "\n",
    "# Function Evaluations per Iteration\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(results)), [val['fevals_per_iteration'] for val in results.values()], color='red', align='center')\n",
    "plt.xticks(range(len(results)), list(results.keys()))\n",
    "plt.title('Function Evaluations per Iteration per Algorithm')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Function Evaluations per Iteration')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a DataFrame from the results dictionary\n",
    "data = {\n",
    "    'Algorithm': [],\n",
    "    'Average Time (s)': [],\n",
    "    'Average Fitness': [],\n",
    "    'Total Function Evaluations': [],\n",
    "    'Fevals per Second': [],\n",
    "    'Fevals per Iteration': []\n",
    "}\n",
    "\n",
    "for algo, metrics in results.items():\n",
    "    data['Algorithm'].append(algo)\n",
    "    data['Average Time (s)'].append(np.mean(metrics['times']))\n",
    "    data['Average Fitness'].append(np.mean(metrics['fitness_scores']))\n",
    "    data['Total Function Evaluations'].append(metrics['total_fevs'])\n",
    "    data['Fevals per Second'].append(metrics['fevals_per_sec'])\n",
    "    data['Fevals per Iteration'].append(metrics['fevals_per_iteration'])\n",
    "\n",
    "results_df_table = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame to check the output\n",
    "results_df_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhc_restarts = [0, 5, 10, 20]\n",
    "rhc_results = {}\n",
    "\n",
    "for restarts in rhc_restarts:\n",
    "    _, best_fitness, _ = mr.random_hill_climb(\n",
    "        problem_4peaks, restarts=restarts, max_attempts=200, max_iters=1000, \n",
    "        curve=True, random_state=42\n",
    "    )\n",
    "    rhc_results[restarts] = best_fitness\n",
    "\n",
    "# You can store these results in a DataFrame or dictionary for later analysis\n",
    "rhc_results_df = pd.DataFrame(rhc_results.items(), columns=['Restarts', 'Best Fitness'])\n",
    "rhc_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for Simulated Annealing parameter tuning\n",
    "sa_temps = [1, 10, 50, 100]\n",
    "sa_decay_consts = [0.001, 0.005, 0.01]\n",
    "\n",
    "sa_results = {}\n",
    "for temp in sa_temps:\n",
    "    for decay in sa_decay_consts:\n",
    "        schedule = mr.ExpDecay(init_temp=temp, exp_const=decay, min_temp=0.001)\n",
    "        _, best_fitness, _ = mr.simulated_annealing(\n",
    "            problem_4peaks, schedule=schedule, max_attempts=200, max_iters=1000, \n",
    "            curve=True, random_state=42\n",
    "        )\n",
    "        sa_results[(temp, decay)] = best_fitness\n",
    "\n",
    "sa_result_df = pd.DataFrame(sa_results.items(), columns=['(Initial Temp, Decay Const)', 'Best Fitness'])\n",
    "sa_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_pop_sizes = [100, 200, 500]\n",
    "ga_mut_probs = [0.05, 0.1, 0.2]\n",
    "ga_results = {}\n",
    "\n",
    "for pop_size in ga_pop_sizes:\n",
    "    for mut_prob in ga_mut_probs:\n",
    "        _, best_fitness, _ = mr.genetic_alg(\n",
    "            problem_4peaks, pop_size=pop_size, mutation_prob=mut_prob,\n",
    "            max_attempts=200, max_iters=1000, curve=True, random_state=42\n",
    "        )\n",
    "        ga_results[(pop_size, mut_prob)] = best_fitness\n",
    "\n",
    "# Store results in a structured format\n",
    "ga_results_df = pd.DataFrame(ga_results.items(), columns=['(Population Size, Mutation Prob)', 'Best Fitness'])\n",
    "ga_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_pop_sizes = [100, 200, 500]\n",
    "mimic_keep_pcts = [0.1, 0.2, 0.3]\n",
    "mimic_results = {}\n",
    "\n",
    "for pop_size in mimic_pop_sizes:\n",
    "    for keep_pct in mimic_keep_pcts:\n",
    "        _, best_fitness, _ = mr.mimic(\n",
    "            problem_4peaks, pop_size=pop_size, keep_pct=keep_pct,\n",
    "            max_attempts=200, max_iters=1000, curve=True, random_state=42\n",
    "        )\n",
    "        mimic_results[(pop_size, keep_pct)] = best_fitness\n",
    "\n",
    "# Format results appropriately for analysis\n",
    "mimic_results_df = pd.DataFrame(mimic_results.items(), columns=['(Population Size, Keep %)', 'Best Fitness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2022.10.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
